{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-core langchain-community faiss-cpu sentence-transformers pypdf\n",
    "%pip install -qU litellm langchain-litellm\n",
    "# 如用 Ollama（本地 LLM），请在系统里先安装并拉取模型：  https://ollama.com\n",
    "# 示例：在终端运行  ollama pull llama3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ded85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 关闭 LangSmith 追踪（避免未配置时报 401/告警）\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "for k in [\"LANGCHAIN_API_KEY\",\"LANGCHAIN_ENDPOINT\",\"LANGCHAIN_PROJECT\"]:\n",
    "    os.environ.pop(k, None)\n",
    "\n",
    "logging.getLogger(\"langchain\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"langsmith\").setLevel(logging.ERROR)\n",
    "\n",
    "# === 选择你的 LLM 方案（二选一） ===\n",
    "USE_GITHUB_MODELS = True    # GitHub Models（免费额度，需 GITHUB_TOKEN）\n",
    "USE_OLLAMA = False          # 本地 LLM（需本机安装 ollama 且已 pull 模型）\n",
    "\n",
    "# GitHub Models 的 PAT（scopes: models 或 models:read）\n",
    "# 建议在系统环境变量里配置 GITHUB_TOKEN；此处读取\n",
    "os.environ[\"GITHUB_TOKEN\"] = os.getenv(\"GITHUB_TOKEN\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a635de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 你的数据目录（可放 txt/md/pdf）\n",
    "DATA_DIR = Path(\"data/private_corpus\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 若目录为空，给一个最小示例文件，方便直接跑通\n",
    "if not any(DATA_DIR.iterdir()):\n",
    "    (DATA_DIR/\"sample.txt\").write_text(\n",
    "        \"孔乙己是站着喝酒而穿长衫的唯一的人。他身材很高大；青白脸色，皱纹间时常夹些伤痕；\"\n",
    "        \"他原来也读过书，但终于没有进学；写得一笔好字，便替人抄抄书，换一碗饭吃。\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "def load_corpus(data_dir: Path):\n",
    "    docs = []\n",
    "    # txt / md\n",
    "    for p in data_dir.rglob(\"*.txt\"):\n",
    "        docs += TextLoader(str(p), encoding=\"utf-8\").load()\n",
    "    for p in data_dir.rglob(\"*.md\"):\n",
    "        docs += TextLoader(str(p), encoding=\"utf-8\").load()\n",
    "    # pdf（可选）\n",
    "    for p in data_dir.rglob(\"*.pdf\"):\n",
    "        docs += PyPDFLoader(str(p)).load()\n",
    "    # 附加来源元信息\n",
    "    for d in docs:\n",
    "        d.metadata[\"source\"] = d.metadata.get(\"source\", d.metadata.get(\"file_path\", \"unknown\"))\n",
    "    return docs\n",
    "\n",
    "raw_docs = load_corpus(DATA_DIR)\n",
    "print(f\"加载原始文档数：{len(raw_docs)}\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700, chunk_overlap=120,\n",
    "    separators=[\"\\n\\n\",\"\\n\",\"。\",\"；\",\"，\",\" \",\"\"]\n",
    ")\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "for i, d in enumerate(docs):\n",
    "    d.metadata[\"chunk_id\"] = i\n",
    "\n",
    "print(f\"切分后文档块数：{len(docs)}\")\n",
    "print(\"示例片段：\", docs[0].page_content[:120])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, emb)\n",
    "\n",
    "# 可选：持久化\n",
    "INDEX_DIR = \"faiss_index_private\"\n",
    "vectorstore.save_local(INDEX_DIR)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "print(\"FAISS 向量库就绪 ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25829014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二选一的 LLM；其余逻辑不变\n",
    "if USE_GITHUB_MODELS:\n",
    "    from langchain_litellm import ChatLiteLLM\n",
    "    GH_TOKEN = os.environ.get(\"GITHUB_TOKEN\", \"\")\n",
    "    if not GH_TOKEN:\n",
    "        raise RuntimeError(\"未找到 GITHUB_TOKEN 环境变量。请设置后重试。\")\n",
    "    llm = ChatLiteLLM(\n",
    "        model=\"openai/gpt-4o-mini\",                 # 可换：openai/gpt-4o, openai/gpt-4.1-mini 等\n",
    "        api_base=\"https://models.github.ai/inference\",  # 关键：GitHub Models 的 /inference 端点\n",
    "        api_key=GH_TOKEN,\n",
    "        temperature=0,\n",
    "    )\n",
    "    print(\"LLM: GitHub Models / gpt-4o-mini\")\n",
    "elif USE_OLLAMA:\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "    llm = ChatOllama(model=\"llama3\", temperature=0)     # 需本机已 ollama pull llama3\n",
    "    print(\"LLM: Ollama / llama3\")\n",
    "else:\n",
    "    raise RuntimeError(\"请至少启用一种 LLM 方案（GitHub Models 或 Ollama）。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "mq = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    include_original=True,  # 加入原始问题\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"你是企业内知识助手。严格遵守：\"\n",
    "    \"1) 仅根据‘检索上下文’作答；若上下文无法支持，回答“我不确定”。\"\n",
    "    \"2) 答案精炼分点。\"\n",
    "    \"3) 末尾附引用列表，格式：[source: 文件名#chunk_id]。\"\n",
    ")\n",
    "\n",
    "QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"用户问题：{question}\\n\\n检索上下文：\\n{context}\\n\\n请作答：\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[{d.metadata.get('chunk_id')}] ({Path(d.metadata.get('source','unknown')).name})\\n{d.page_content}\"\n",
    "        for d in docs\n",
    "    )\n",
    "\n",
    "def citations(docs):\n",
    "    outs = []\n",
    "    for d in docs:\n",
    "        src = Path(d.metadata.get(\"source\",\"unknown\")).name\n",
    "        cid = d.metadata.get(\"chunk_id\")\n",
    "        outs.append(f\"[source: {src}#{cid}]\")\n",
    "    # 去重并保持稳定顺序\n",
    "    return list(dict.fromkeys(outs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel, RunnablePassthrough, RunnableLambda, RunnableWithMessageHistory\n",
    ")\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# ===== 会话历史（内存）=====\n",
    "SESSION_STORE = {}\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in SESSION_STORE:\n",
    "        SESSION_STORE[session_id] = InMemoryChatMessageHistory()\n",
    "    return SESSION_STORE[session_id]\n",
    "\n",
    "def history_factory(session_id: str):\n",
    "    return get_session_history(session_id)\n",
    "\n",
    "# ===== Prompt =====\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一名助手。基于对话历史，将用户问题改写为独立、具体、可检索的问题。只输出改写后的问题。\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是企业内知识助手。仅依据‘检索上下文’作答；无法确定就回答“我不确定”。答案精炼分点，末尾列出来源 [source: 文件名#chunk]。\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"用户问题：{question}\\n\\n检索上下文：\\n{context}\\n\\n请作答：\"),\n",
    "])\n",
    "\n",
    "rewrite_chain = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[{d.metadata.get('chunk_id')}] ({Path(d.metadata.get('source','unknown')).name})\\n{d.page_content}\"\n",
    "        for d in docs\n",
    "    )\n",
    "\n",
    "def citations(docs):\n",
    "    outs = []\n",
    "    for d in docs:\n",
    "        src = Path(d.metadata.get(\"source\",\"unknown\")).name\n",
    "        cid = d.metadata.get(\"chunk_id\")\n",
    "        outs.append(f\"[source: {src}#{cid}]\")\n",
    "    # 去重并稳定顺序\n",
    "    return \" \".join(list(dict.fromkeys(outs)))\n",
    "\n",
    "# ===== 构建 rag_core：改写 → 多查询召回 → 组装上下文/引用 → 回答 → 拼接引用 =====\n",
    "rag_core = (\n",
    "    RunnableParallel(\n",
    "        # 原问题直接透传；history 需要显式取列表：x[\"history\"]\n",
    "        question = RunnablePassthrough(),\n",
    "        history  = RunnableLambda(lambda x: x[\"history\"]),\n",
    "        # 改写需要 question + history\n",
    "        rewritten_question = rewrite_chain,\n",
    "    )\n",
    "    # 1) 基于改写后的问题做 MultiQuery 召回\n",
    "    | RunnableLambda(lambda x: {**x, \"docs\": mq.invoke(x[\"rewritten_question\"])})\n",
    "    # 2) 形成上下文与引用文本\n",
    "    | RunnableLambda(lambda x: {**x, \"context\": format_docs(x[\"docs\"]), \"cites\": citations(x[\"docs\"])})\n",
    "    # 3) 送入回答 Prompt\n",
    "    | QA_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    # 4) 与引用拼接\n",
    "    | RunnableLambda(lambda answer, **kw: answer + (f\"\\n\\n参考来源： {kw.get('cites','')}\" if kw.get('cites') else \"\"))\n",
    ").with_config(run_name=\"RAG-Core\")\n",
    "\n",
    "# ===== 包装成“带记忆”的链 =====\n",
    "rag_with_memory = RunnableWithMessageHistory(\n",
    "    rag_core,\n",
    "    history_factory,                 # 只接收 session_id: str\n",
    "    input_messages_key=\"question\",   # 本轮输入的字段\n",
    "    history_messages_key=\"history\",  # 注入到两个 Prompt 的占位符\n",
    ")\n",
    "\n",
    "print(\"✅ RAG + 记忆 链已就绪\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\"configurable\": {\"session_id\": \"user-001\"}}\n",
    "\n",
    "print(\"Q1:\")\n",
    "print(\n",
    "    rag_with_memory.invoke({\"question\": \"根据语料，孔乙己的性格特点是什么？\"}, config=cfg)\n",
    ")\n",
    "\n",
    "print(\"\\nQ2（指代上一轮）：\")\n",
    "print(\n",
    "    rag_with_memory.invoke({\"question\": \"他有哪些典型行为可以作为证据？\"}, config=cfg)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19366d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2 = {\"configurable\": {\"session_id\": \"user-002\"}}\n",
    "\n",
    "print(\"Q1:\")\n",
    "print(\n",
    "    rag_with_memory.invoke({\"question\": \"My name is Yongjun Wang, I am from China\"}, config=cfg2)\n",
    ")\n",
    "\n",
    "print(\"\\nQ2（指代上一轮）：\")\n",
    "print(\n",
    "    rag_with_memory.invoke({\"question\": \"What am I from?\"}, config=cfg2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2 = {\"configurable\": {\"session_id\": \"user-002\"}}\n",
    "\n",
    "print(\"Q1:\")\n",
    "print(\n",
    "    rag_with_memory.invoke({\"question\": \"if A + B = 3A\"}, config=cfg2)\n",
    ")\n",
    "\n",
    "print(\"\\nQ2（指代上一轮）：\")\n",
    "print(\n",
    "    rag_with_memory.invoke({\"question\": \"So, what does B equal?\"}, config=cfg2)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
