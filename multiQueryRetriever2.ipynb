{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-community faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a065beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, logging\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ.pop(\"LANGCHAIN_API_KEY\", None)\n",
    "os.environ.pop(\"LANGCHAIN_ENDPOINT\", None)\n",
    "os.environ.pop(\"LANGCHAIN_PROJECT\", None)\n",
    "\n",
    "# 可选：降低日志噪声\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "logging.getLogger(\"langchain\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"langsmith\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"GITHUB_TOKEN\"):\n",
    "    print(\"Error: GITHUB_TOKEN is not set in .env file\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "os.environ[\"GITHUB_TOKEN\"] = os.getenv(\"GITHUB_TOKEN\")\n",
    "os.environ[\"GITHUB_MODEL\"] = os.getenv(\"GITHUB_MODEL\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 可选：减少不必要的日志\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# langChain tex loader\n",
    "loader = TextLoader(\"data/kongyiji.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"加载了 {len(docs)} 个 Document\")\n",
    "print(docs[0].page_content[:200])  # 看一下前200字符\n",
    "print(docs[0].metadata)  # 看一下元数据\n",
    "#print(docs[0].page_content)  # 看一下后200字符\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4038cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=80,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"，\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"切分后共 {len(split_docs)} 段\")\n",
    "print(split_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbf80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(split_docs, emb)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "print(\"向量库构建完成 ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb378da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # 这里可以替换成 github models 的名称\n",
    "    temperature=0,\n",
    "    base_url=\"https://models.github.ai/inference\",  # 如果用 GitHub Models 要加上\n",
    "    api_key=os.environ[\"GITHUB_TOKEN\"]\n",
    ")\n",
    "\n",
    "mq = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm, include_original=True)\n",
    "\n",
    "query = \"根据文本，谈一下孔乙己的性格特点\"\n",
    "results = mq.get_relevant_documents(query)\n",
    "\n",
    "print(f\"检索到 {len(results)} 个文档片段：\")\n",
    "for i, d in enumerate(results, 1):\n",
    "    print(f\"[{i}] {d.page_content[:100]}...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
