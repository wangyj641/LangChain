{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-community faiss-cpu sentence-transformers langchain-litellm litellm pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "for k in [\"LANGCHAIN_API_KEY\",\"LANGCHAIN_ENDPOINT\",\"LANGCHAIN_PROJECT\"]:\n",
    "    os.environ.pop(k, None)\n",
    "\n",
    "logging.getLogger(\"langchain\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"langsmith\").setLevel(logging.ERROR)\n",
    "\n",
    "# 选择 LLM 方案（二选一）：\n",
    "USE_OLLAMA = False           # 本地 LLM（需本机已安装 ollama 并拉取模型）\n",
    "USE_GITHUB_MODELS = True     # GitHub Models（免费额度；需 GITHUB_TOKEN）\n",
    "\n",
    "# GitHub Models 的 PAT（scopes: models 或 models:read）\n",
    "os.environ[\"GITHUB_TOKEN\"] = os.getenv(\"GITHUB_TOKEN\", \"ghp_xxx_put_your_token_here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 你的私域数据目录（可混放 txt/md/pdf）\n",
    "DATA_DIR = Path(\"data/private_corpus\")\n",
    "\n",
    "def load_corpus(data_dir: Path):\n",
    "    docs = []\n",
    "    # txt/md\n",
    "    for p in data_dir.rglob(\"*.txt\"):\n",
    "        docs += TextLoader(str(p), encoding=\"utf-8\").load()\n",
    "    for p in data_dir.rglob(\"*.md\"):\n",
    "        docs += TextLoader(str(p), encoding=\"utf-8\").load()\n",
    "    # pdf（可选）\n",
    "    for p in data_dir.rglob(\"*.pdf\"):\n",
    "        docs += PyPDFLoader(str(p)).load()\n",
    "    return docs\n",
    "\n",
    "raw_docs = load_corpus(DATA_DIR)\n",
    "print(f\"加载原始文档数：{len(raw_docs)}\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=60, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"；\", \"，\", \" \", \"\"]\n",
    ")\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "# 附加来源标记\n",
    "for i, d in enumerate(docs):\n",
    "    d.metadata[\"chunk_id\"] = i\n",
    "\n",
    "print(f\"切分后文档块数：{len(docs)}\")\n",
    "print(docs[0].metadata, docs[0].page_content[:120], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, emb)\n",
    "\n",
    "# 可选：持久化\n",
    "INDEX_DIR = \"faiss_index_private\"\n",
    "vectorstore.save_local(INDEX_DIR)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "print(\"FAISS 索引构建完成 ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseMessage\n",
    "from typing import List\n",
    "\n",
    "# A) 使用 Ollama, 需本机已安装并拉取模型\n",
    "if USE_OLLAMA:\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "    llm = ChatOllama(model=\"llama3\", temperature=0)   # 可换 qwen2.5, mistral 等\n",
    "    print(\"LLM: Ollama/llama3\")\n",
    "\n",
    "# B) 使用 GitHub Models（OpenAI 兼容；通过 LiteLLM 确保 /inference 路径）\n",
    "if USE_GITHUB_MODELS:\n",
    "    from langchain_litellm import ChatLiteLLM\n",
    "    import os\n",
    "    llm = ChatLiteLLM(\n",
    "        model=\"openai/gpt-4o-mini\",                         # 到 GitHub Models 目录可替换别的模型\n",
    "        api_base=\"https://models.github.ai/inference\",      # 关键：/inference\n",
    "        api_key=os.environ[\"GITHUB_TOKEN\"],\n",
    "        temperature=0,\n",
    "    )\n",
    "    print(\"LLM: GitHub Models / gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f24ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "mq = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    include_original=True,  # 原始问题也参与检索\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"你是一个严谨的企业内知識助手。严格遵守：\\n\"\n",
    "    \"1) **仅根据提供的上下文**回答；若无法从上下文中确定答案，必须回答“我不确定”。\\n\"\n",
    "    \"2) 回答尽量简洁、分点描述。\\n\"\n",
    "    \"3) 在答案末尾给出引用列表，格式：[source: 文件名#chunk_id]，可包含多个。\\n\"\n",
    ")\n",
    "\n",
    "QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"用户问题：{question}\\n\\n上下文：\\n{context}\\n\\n请作答：\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 传给模型阅读的上下文\n",
    "    return \"\\n\\n\".join(\n",
    "        [f\"[{d.metadata.get('chunk_id')}] ({d.metadata.get('source','unknown')})\\n{d.page_content}\"\n",
    "         for d in docs]\n",
    "    )\n",
    "\n",
    "def citations(docs):\n",
    "    # 供模型在答案后部附带引用\n",
    "    outs = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\", \"unknown\")\n",
    "        cid = d.metadata.get(\"chunk_id\")\n",
    "        outs.append(f\"[source: {Path(src).name}#{cid}]\")\n",
    "    # 去重\n",
    "    return sorted(set(outs), key=lambda x: x)\n",
    "\n",
    "# RAG 链（按需可替换为历史对话版本）\n",
    "def rag_answer(question: str) -> str:\n",
    "    # 多查询召回 + 去重\n",
    "    top_docs = mq.invoke(question)\n",
    "    if not top_docs:\n",
    "        return \"我不确定。未在知识库中检索到相关内容。\"\n",
    "\n",
    "    ctx = format_docs(top_docs)\n",
    "    answer = (QA_PROMPT | llm | StrOutputParser()).invoke({\"question\": question, \"context\": ctx})\n",
    "\n",
    "    # 追加引用（稳妥起见也可让模型自己生成）\n",
    "    cites = \" \".join(citations(top_docs))\n",
    "    if cites and cites not in answer:\n",
    "        answer = f\"{answer}\\n\\n参考来源：{cites}\"\n",
    "    return answer\n",
    "\n",
    "print(\"RAG Bot 就绪 ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af42e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"根据语料，三星电子晋升评估中, 绩效占比多少?并给出证据。\"\n",
    "print(rag_answer(q))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
